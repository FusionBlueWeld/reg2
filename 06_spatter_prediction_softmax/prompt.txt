私はレーザ溶接の研究者です。
大量の溶接データが得られたので、ディープラーニングを用いて、溶接加工パラメータの予測器を作成用と思います。

説明変数は、
１．加工ヘッド位置（-6.0mmから、+6.0mmまで）
２．溶接速度（30mm/secから、600mm/secまで）

であり、目的変数は「スパッタ量」です。スパッタ量は0から1の間の値で表現され、
「1（多い）～0（少ない）」
という関係です。

実測データをまとめた学習データセットは、CSVファイルで、
１列目．加工ヘッド位置（-6.0mmから、+6.0mmまで）
２列目．溶接速度（30mm/secから、600mm/secまで）
３列目．スパッタ量（0から、1まで）

という構成で用意されています。1行目はヘッダーラベル、2行目からデータ値で、データは65点（65行分）あります。


# ファイルを構成するリポジトリ構造
04_spatter_prediction/
│
├───input_data/
│       input_data.csv
│
├───output_data/
│       trained_welding_model.h5
│
└───src/
        gui.py
        main.py
        output.py
        predict.py
        regression.py

gui.py: 溶接モデルを使用した予測結果をGUIで表示するPythonファイル。
main.py: 各ファイルを実行し、結果を取りまとめるPythonファイル。
output.py: グラフやCSVなどの出力を取りまとめたPythonファイル。
predict.py: 学習済みの溶接モデルを使用して、新しいデータに対して予測を行うPythonファイル。
regression.py: 実測のデータから回帰分析を行い、予測モデルを学習するPythonファイル。

input_data.csvが学習データセットあたるファイルです。
output_data/trained_welding_model.h5は学習済のモデルを保存するときのファイル名です。

2つの説明変数、
１．加工ヘッド位置（-6.0mmから、+6.0mmまで）
２．溶接速度（30mm/secから、600mm/secまで）
の範囲で、実測結果にフィットするモデルを作成したいです。
この条件の下で最適と思われる、ニューラルネットワークのアーキテクチャを提案してください。


#－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－

まずは、regression.pyのコードを下記のように書きました。
更に、認識した上で、ここまでの会話で使用しているトークン数を教えてください。

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import tkinter as tk
from tkinter import filedialog
import matplotlib.pyplot as plt

# GPUを使用しない設定
tf.config.set_visible_devices([], 'GPU')

# ランダムシードの設定
tf.random.set_seed(42)

# ファイル選択ダイアログの表示
root = tk.Tk()
root.withdraw()
file_path = filedialog.askopenfilename(filetypes=[("CSV files", "*.csv")])

# データの読み込み
data = pd.read_csv(file_path)

# 入力データとターゲットデータに分割
X = data.iloc[:, :4].values
y = data.iloc[:, 4].values

# データの正規化
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# データを訓練用とテスト用に分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# モデルの定義
model = Sequential([
    Dense(128, activation='relu', input_shape=(4,)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# モデルのコンパイル
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# モデルの学習
history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_split=0.2)

# モデルの評価
loss, mae = model.evaluate(X_test, y_test)
print('Test Loss:', loss)
print('Test MAE:', mae)

# グラフの描画
fig, ax1 = plt.subplots()
ax2 = ax1.twinx()

ax1.plot(history.history['loss'], label='Training Loss', color='b')
ax1.plot(history.history['val_loss'], label='Validation Loss', color='b', linestyle='dashed')
ax1.set_xlabel('Epochs')
ax1.set_ylabel('Loss')
ax1.set_ylim(bottom=0, top=0.5)

ax2.plot(history.history['mae'], label='Training MAE', color='r')
ax2.plot(history.history['val_mae'], label='Validation MAE', color='r', linestyle='dashed')
ax2.set_ylabel('MAE')
ax2.set_ylim(bottom=0, top=0.5)

# 凡例とグリッド線の設定
fig.legend(loc='upper right', bbox_to_anchor=(0.85, 0.9))

ax1.grid(True)
ax2.grid(True)

plt.title('Loss and MAE')

# グラフを枠内に収める
plt.tight_layout()

# グラフ画像を同じフォルダに保存
plt.savefig('loss_and_mae.png')

# モデルの保存
model.save('trained_welding_model.h5')


#－－－－－－－－－－－－－－－－－－－－－－－－－－－－－－

次にこのファイルを下記の機能で2つに分けたいです。
１．回帰分析（モデル保存含む）
⇒regression.pyに記述

２．グラフの出力
⇒output.pyに記述

regression.pyから、output.pyのこのグラフ化機能を呼び出して、使用する構成としてください。

このregression.py、およびoutput.pyのコードを記述してください。
